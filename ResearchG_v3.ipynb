{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aif360'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m product\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maif360\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryLabelDatasetMetric, utils\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maif360\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryLabelDataset\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maif360\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass_label_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassLabelDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aif360'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, utils\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets.multiclass_label_dataset import MulticlassLabelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_german(df, preprocess):\n",
    "    df['status'] = df['status'].map({'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}).astype(int)\n",
    "    df['credit_hist'] = df['credit_hist'].map({'A34': 0, 'A33': 1, 'A32': 2, 'A31': 3, 'A30': 4}).astype(int)\n",
    "    df.loc[(df['credit_amt'] <= 2000), 'credit_amt'] = 0\n",
    "    df.loc[(df['credit_amt'] > 2000) & (df['credit_amt'] <= 5000), 'credit_amt'] = 1\n",
    "    df.loc[(df['credit_amt'] > 5000), 'credit_amt'] = 2    \n",
    "    df.loc[(df['duration'] <= 12), 'duration'] = 0\n",
    "    df.loc[(df['duration'] > 12) & (df['duration'] <= 24), 'duration'] = 1\n",
    "    df.loc[(df['duration'] > 24) & (df['duration'] <= 36), 'duration'] = 2\n",
    "    df.loc[(df['duration'] > 36), 'duration'] = 3\n",
    "    df['age'] = df['age'].apply(lambda x : 1 if x >= 45 else 0) # 1 if old, 0 if young\n",
    "\n",
    "    df['savings'] = df['savings'].map({'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}).astype(int)\n",
    "    df['employment'] = df['employment'].map({'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}).astype(int)    \n",
    "    df['gender'] = df['personal_status'].map({'A91': 1, 'A92': 0, 'A93': 1, 'A94': 1, 'A95': 0}).astype(int)\n",
    "    df['debtors'] = df['debtors'].map({'A101': 0, 'A102': 1, 'A103': 2}).astype(int)\n",
    "    df['property'] = df['property'].map({'A121': 3, 'A122': 2, 'A123': 1, 'A124': 0}).astype(int)        \n",
    "    df['install_plans'] = df['install_plans'].map({'A141': 1, 'A142': 1, 'A143': 0}).astype(int)\n",
    "    if preprocess:\n",
    "        df = pd.concat([df, pd.get_dummies(df['purpose'], prefix='purpose')],axis=1)\n",
    "        df = pd.concat([df, pd.get_dummies(df['housing'], prefix='housing')],axis=1)\n",
    "    df['job'] = df['job'].map({'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}).astype(int)    \n",
    "    df['telephone'] = df['telephone'].map({'A191': 0, 'A192': 1}).astype(int)\n",
    "    df['foreign_worker'] = df['foreign_worker'].map({'A201': 1, 'A202': 0}).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_german(preprocess=True):\n",
    "    cols = ['status', 'duration', 'credit_hist', 'purpose', 'credit_amt', 'savings', 'employment',\\\n",
    "            'install_rate', 'personal_status', 'debtors', 'residence', 'property', 'age', 'install_plans',\\\n",
    "            'housing', 'num_credits', 'job', 'num_liable', 'telephone', 'foreign_worker', 'credit']\n",
    "    df = pd.read_table('german.data', names=cols, sep=\" \", index_col=False)\n",
    "    df['credit'] = df['credit'].replace(2, 0) #1 = Good, 2= Bad credit risk\n",
    "    y = df['credit']\n",
    "    df = preprocess_german(df, preprocess)\n",
    "    if preprocess:\n",
    "        df = df.drop(columns=['purpose', 'personal_status', 'housing', 'credit'])\n",
    "    else:\n",
    "        df = df.drop(columns=['personal_status', 'credit'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=1)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_german(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationMetric(BinaryLabelDatasetMetric):\n",
    "    \"\"\"Class for computing metrics based on two BinaryLabelDatasets.\n",
    "    The first dataset is the original one and the second is the output of the\n",
    "    classification transformer (or similar).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, classified_dataset,\n",
    "                 unprivileged_groups=None, privileged_groups=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing ground-truth\n",
    "                labels.\n",
    "            classified_dataset (BinaryLabelDataset): Dataset containing\n",
    "                predictions.\n",
    "            privileged_groups (list(dict)): Privileged groups. Format is a list\n",
    "                of `dicts` where the keys are `protected_attribute_names` and\n",
    "                the values are values in `protected_attributes`. Each `dict`\n",
    "                element describes a single group. See examples for more details.\n",
    "            unprivileged_groups (list(dict)): Unprivileged groups in the same\n",
    "                format as `privileged_groups`.\n",
    "        Raises:\n",
    "            TypeError: `dataset` and `classified_dataset` must be\n",
    "                :obj:`~aif360.datasets.BinaryLabelDataset` types.\n",
    "        \"\"\"\n",
    "        if not isinstance(dataset, BinaryLabelDataset) and not isinstance(dataset, MulticlassLabelDataset) :\n",
    "            raise TypeError(\"'dataset' should be a BinaryLabelDataset or a MulticlassLabelDataset\")\n",
    "\n",
    "        # sets self.dataset, self.unprivileged_groups, self.privileged_groups\n",
    "        super(ClassificationMetric, self).__init__(dataset,\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "\n",
    "        if isinstance(classified_dataset, BinaryLabelDataset) or isinstance(classified_dataset, MulticlassLabelDataset) :\n",
    "            self.classified_dataset = classified_dataset\n",
    "        else:\n",
    "            raise TypeError(\"'classified_dataset' should be a \"\n",
    "                            \"BinaryLabelDataset or a MulticlassLabelDataset.\")\n",
    "\n",
    "        if isinstance(self.classified_dataset, MulticlassLabelDataset):\n",
    "            fav_label_value = 1.\n",
    "            unfav_label_value = 0.\n",
    "\n",
    "            self.classified_dataset = self.classified_dataset.copy()\n",
    "            # Find all the labels which match any of the favorable labels\n",
    "            fav_idx = np.logical_or.reduce(np.equal.outer(self.classified_dataset.favorable_label, self.classified_dataset.labels))\n",
    "            # Replace labels with corresponding values\n",
    "            self.classified_dataset.labels = np.where(fav_idx, fav_label_value, unfav_label_value)\n",
    "            \n",
    "            self.classified_dataset.favorable_label = float(fav_label_value)\n",
    "            self.classified_dataset.unfavorable_label = float(unfav_label_value)\n",
    "        \n",
    "        # Verify if everything except the predictions and metadata are the same\n",
    "        # for the two datasets\n",
    "        with self.dataset.temporarily_ignore('labels', 'scores'):\n",
    "            if self.dataset != self.classified_dataset:\n",
    "                raise ValueError(\"The two datasets are expected to differ only \"\n",
    "                                 \"in 'labels' or 'scores'.\")\n",
    "\n",
    "    def binary_confusion_matrix(self, privileged=None):\n",
    "        \"\"\"Compute the number of true/false positives/negatives, optionally\n",
    "        conditioned on protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Returns:\n",
    "            dict: Number of true positives, false positives, true negatives,\n",
    "            false negatives (optionally conditioned).\n",
    "        \"\"\"\n",
    "        condition = self._to_condition(privileged)\n",
    "\n",
    "        return utils.compute_num_TF_PN(self.dataset.protected_attributes,\n",
    "            self.dataset.labels, self.classified_dataset.labels,\n",
    "            self.dataset.instance_weights,\n",
    "            self.dataset.protected_attribute_names,\n",
    "            self.dataset.favorable_label, self.dataset.unfavorable_label,\n",
    "            condition=condition)\n",
    "\n",
    "    def generalized_binary_confusion_matrix(self, privileged=None):\n",
    "        \"\"\"Compute the number of generalized true/false positives/negatives,\n",
    "        optionally conditioned on protected attributes. Generalized counts are\n",
    "        based on scores and not on the hard predictions.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Returns:\n",
    "            dict: Number of generalized true positives, generalized false\n",
    "            positives, generalized true negatives, generalized false negatives\n",
    "            (optionally conditioned).\n",
    "        \"\"\"\n",
    "        condition = self._to_condition(privileged)\n",
    "\n",
    "        return utils.compute_num_gen_TF_PN(self.dataset.protected_attributes,\n",
    "            self.dataset.labels, self.classified_dataset.scores,\n",
    "            self.dataset.instance_weights,\n",
    "            self.dataset.protected_attribute_names,\n",
    "            self.dataset.favorable_label, self.dataset.unfavorable_label,\n",
    "            condition=condition)\n",
    "\n",
    "    def num_true_positives(self, privileged=None):\n",
    "        r\"\"\"Return the number of instances in the dataset where both the\n",
    "        predicted and true labels are 'favorable',\n",
    "        :math:`TP = \\sum_{i=1}^n \\mathbb{1}[y_i = \\text{favorable}]\\mathbb{1}[\\hat{y}_i = \\text{favorable}]`,\n",
    "        optionally conditioned on protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups`\n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.binary_confusion_matrix(privileged=privileged)['TP']\n",
    "\n",
    "    def num_false_positives(self, privileged=None):\n",
    "        r\"\"\":math:`FP = \\sum_{i=1}^n \\mathbb{1}[y_i = \\text{unfavorable}]\\mathbb{1}[\\hat{y}_i = \\text{favorable}]`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.binary_confusion_matrix(privileged=privileged)['FP']\n",
    "\n",
    "    def num_false_negatives(self, privileged=None):\n",
    "        r\"\"\":math:`FN = \\sum_{i=1}^n \\mathbb{1}[y_i = \\text{favorable}]\\mathbb{1}[\\hat{y}_i = \\text{unfavorable}]`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups`\n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.binary_confusion_matrix(privileged=privileged)['FN']\n",
    "\n",
    "    def num_true_negatives(self, privileged=None):\n",
    "        r\"\"\":math:`TN = \\sum_{i=1}^n \\mathbb{1}[y_i = \\text{unfavorable}]\\mathbb{1}[\\hat{y}_i = \\text{unfavorable}]`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups`\n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.binary_confusion_matrix(privileged=privileged)['TN']\n",
    "\n",
    "    def num_generalized_true_positives(self, privileged=None):\n",
    "        \"\"\"Return the generalized number of true positives, :math:`GTP`, the\n",
    "        weighted sum of predicted scores where true labels are 'favorable',\n",
    "        optionally conditioned on protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.generalized_binary_confusion_matrix(\n",
    "            privileged=privileged)['GTP']\n",
    "\n",
    "    def num_generalized_false_positives(self, privileged=None):\n",
    "        \"\"\"Return the generalized number of false positives, :math:`GFP`, the\n",
    "        weighted sum of predicted scores where true labels are 'unfavorable',\n",
    "        optionally conditioned on protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` must be\n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.generalized_binary_confusion_matrix(\n",
    "            privileged=privileged)['GFP']\n",
    "\n",
    "    def num_generalized_false_negatives(self, privileged=None):\n",
    "        \"\"\"Return the generalized number of false negatives, :math:`GFN`, the\n",
    "        weighted sum of 1 - predicted scores where true labels are 'favorable',\n",
    "        optionally conditioned on protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups`\n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.generalized_binary_confusion_matrix(\n",
    "            privileged=privileged)['GFN']\n",
    "\n",
    "    def num_generalized_true_negatives(self, privileged=None):\n",
    "        \"\"\"Return the generalized number of true negatives, :math:`GTN`, the\n",
    "        weighted sum of 1 - predicted scores where true labels are 'unfavorable',\n",
    "        optionally conditioned on protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.generalized_binary_confusion_matrix(\n",
    "            privileged=privileged)['GTN']\n",
    "\n",
    "    def performance_measures(self, privileged=None):\n",
    "        \"\"\"Compute various performance measures on the dataset, optionally\n",
    "        conditioned on protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Returns:\n",
    "            dict: True positive rate, true negative rate, false positive rate,\n",
    "            false negative rate, positive predictive value, negative predictive\n",
    "            value, false discover rate, false omission rate, and accuracy\n",
    "            (optionally conditioned).\n",
    "        \"\"\"\n",
    "        TP = self.num_true_positives(privileged=privileged)\n",
    "        FP = self.num_false_positives(privileged=privileged)\n",
    "        FN = self.num_false_negatives(privileged=privileged)\n",
    "        TN = self.num_true_negatives(privileged=privileged)\n",
    "        GTP = self.num_generalized_true_positives(privileged=privileged)\n",
    "        GFP = self.num_generalized_false_positives(privileged=privileged)\n",
    "        GFN = self.num_generalized_false_negatives(privileged=privileged)\n",
    "        GTN = self.num_generalized_true_negatives(privileged=privileged)\n",
    "        P = self.num_positives(privileged=privileged)\n",
    "        N = self.num_negatives(privileged=privileged)\n",
    "\n",
    "        return dict(\n",
    "            TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
    "            GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
    "            PPV=TP / (TP+FP) if (TP+FP) > 0.0 else np.float64(0.0),\n",
    "            NPV=TN / (TN+FN) if (TN+FN) > 0.0 else np.float64(0.0),\n",
    "            FDR=FP / (FP+TP) if (FP+TP) > 0.0 else np.float64(0.0),\n",
    "            FOR=FN / (FN+TN) if (FN+TN) > 0.0 else np.float64(0.0),\n",
    "            ACC=(TP+TN) / (P+N) if (P+N) > 0.0 else np.float64(0.0)\n",
    "        )\n",
    "\n",
    "    def true_positive_rate(self, privileged=None):\n",
    "        \"\"\"Return the ratio of true positives to positive examples in the\n",
    "        dataset, :math:`TPR = TP/P`, optionally conditioned on protected\n",
    "        attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['TPR']\n",
    "\n",
    "    def false_positive_rate(self, privileged=None):\n",
    "        \"\"\":math:`FPR = FP/N`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['FPR']\n",
    "\n",
    "    def false_negative_rate(self, privileged=None):\n",
    "        \"\"\":math:`FNR = FN/P`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups`\n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['FNR']\n",
    "\n",
    "    def true_negative_rate(self, privileged=None):\n",
    "        \"\"\":math:`TNR = TN/N`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['TNR']\n",
    "\n",
    "    def generalized_true_positive_rate(self, privileged=None):\n",
    "        \"\"\"Return the ratio of generalized true positives to positive examples\n",
    "        in the dataset, :math:`GTPR = GTP/P`, optionally conditioned on\n",
    "        protected attributes.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['GTPR']\n",
    "\n",
    "    def generalized_false_positive_rate(self, privileged=None):\n",
    "        \"\"\":math:`GFPR = GFP/N`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['GFPR']\n",
    "\n",
    "    def generalized_false_negative_rate(self, privileged=None):\n",
    "        \"\"\":math:`GFNR = GFN/P`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups`\n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['GFNR']\n",
    "\n",
    "    def generalized_true_negative_rate(self, privileged=None):\n",
    "        \"\"\":math:`GTNR = GTN/N`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['GTNR']\n",
    "\n",
    "    def positive_predictive_value(self, privileged=None):\n",
    "        \"\"\":math:`PPV = TP/(TP + FP)`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['PPV']\n",
    "\n",
    "    def false_discovery_rate(self, privileged=None):\n",
    "        \"\"\":math:`FDR = FP/(TP + FP)`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['FDR']\n",
    "\n",
    "    def false_omission_rate(self, privileged=None):\n",
    "        \"\"\":math:`FOR = FN/(TN + FN)`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['FOR']\n",
    "\n",
    "    def negative_predictive_value(self, privileged=None):\n",
    "        \"\"\":math:`NPV = TN/(TN + FN)`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['NPV']\n",
    "\n",
    "    def accuracy(self, privileged=None):\n",
    "        \"\"\":math:`ACC = (TP + TN)/(P + N)`.\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return self.performance_measures(privileged=privileged)['ACC']\n",
    "\n",
    "    def error_rate(self, privileged=None):\n",
    "        \"\"\":math:`ERR = (FP + FN)/(P + N)`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return 1. - self.accuracy(privileged=privileged)\n",
    "\n",
    "    def true_positive_rate_difference(self):\n",
    "        r\"\"\":math:`TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}`\n",
    "        \"\"\"\n",
    "        return self.difference(self.true_positive_rate)\n",
    "\n",
    "    def false_positive_rate_difference(self):\n",
    "        r\"\"\":math:`FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}}`\n",
    "        \"\"\"\n",
    "        return self.difference(self.false_positive_rate)\n",
    "\n",
    "    def false_negative_rate_difference(self):\n",
    "        r\"\"\":math:`FNR_{D = \\text{unprivileged}} - FNR_{D = \\text{privileged}}`\n",
    "        \"\"\"\n",
    "        return self.difference(self.false_negative_rate)\n",
    "\n",
    "    def false_omission_rate_difference(self):\n",
    "        r\"\"\":math:`FOR_{D = \\text{unprivileged}} - FOR_{D = \\text{privileged}}`\n",
    "        \"\"\"\n",
    "        return self.difference(self.false_omission_rate)\n",
    "\n",
    "    def false_discovery_rate_difference(self):\n",
    "        r\"\"\":math:`FDR_{D = \\text{unprivileged}} - FDR_{D = \\text{privileged}}`\n",
    "        \"\"\"\n",
    "        return self.difference(self.false_discovery_rate)\n",
    "\n",
    "    def false_positive_rate_ratio(self):\n",
    "        r\"\"\":math:`\\frac{FPR_{D = \\text{unprivileged}}}{FPR_{D = \\text{privileged}}}`\n",
    "        \"\"\"\n",
    "        return self.ratio(self.false_positive_rate)\n",
    "\n",
    "    def false_negative_rate_ratio(self):\n",
    "        r\"\"\":math:`\\frac{FNR_{D = \\text{unprivileged}}}{FNR_{D = \\text{privileged}}}`\n",
    "        \"\"\"\n",
    "        return self.ratio(self.false_negative_rate)\n",
    "\n",
    "    def false_omission_rate_ratio(self):\n",
    "        r\"\"\":math:`\\frac{FOR_{D = \\text{unprivileged}}}{FOR_{D = \\text{privileged}}}`\n",
    "        \"\"\"\n",
    "        return self.ratio(self.false_omission_rate)\n",
    "\n",
    "    def false_discovery_rate_ratio(self):\n",
    "        r\"\"\":math:`\\frac{FDR_{D = \\text{unprivileged}}}{FDR_{D = \\text{privileged}}}`\n",
    "        \"\"\"\n",
    "        return self.ratio(self.false_discovery_rate)\n",
    "\n",
    "    def average_odds_difference(self):\n",
    "        r\"\"\"Average of difference in FPR and TPR for unprivileged and privileged\n",
    "        groups:\n",
    "        .. math::\n",
    "           \\tfrac{1}{2}\\left[(FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}})\n",
    "           + (TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}))\\right]\n",
    "        A value of 0 indicates equality of odds.\n",
    "        \"\"\"\n",
    "        return 0.5 * (self.difference(self.false_positive_rate)\n",
    "                    + self.difference(self.true_positive_rate))\n",
    "\n",
    "    def average_abs_odds_difference(self):\n",
    "        r\"\"\"Average of absolute difference in FPR and TPR for unprivileged and\n",
    "        privileged groups:\n",
    "        .. math::\n",
    "           \\tfrac{1}{2}\\left[|FPR_{D = \\text{unprivileged}} - FPR_{D = \\text{privileged}}|\n",
    "           + |TPR_{D = \\text{unprivileged}} - TPR_{D = \\text{privileged}}|\\right]\n",
    "        A value of 0 indicates equality of odds.\n",
    "        \"\"\"\n",
    "        return 0.5 * (np.abs(self.difference(self.false_positive_rate))\n",
    "                    + np.abs(self.difference(self.true_positive_rate)))\n",
    "\n",
    "    def error_rate_difference(self):\n",
    "        r\"\"\"Difference in error rates for unprivileged and privileged groups,\n",
    "        :math:`ERR_{D = \\text{unprivileged}} - ERR_{D = \\text{privileged}}`.\n",
    "        \"\"\"\n",
    "        return self.difference(self.error_rate)\n",
    "\n",
    "    def error_rate_ratio(self):\n",
    "        r\"\"\"Ratio of error rates for unprivileged and privileged groups,\n",
    "        :math:`\\frac{ERR_{D = \\text{unprivileged}}}{ERR_{D = \\text{privileged}}}`.\n",
    "        \"\"\"\n",
    "        return self.ratio(self.error_rate)\n",
    "\n",
    "    def num_pred_positives(self, privileged=None):\n",
    "        r\"\"\":math:`\\sum_{i=1}^n \\mathbb{1}[\\hat{y}_i = \\text{favorable}]`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        condition = self._to_condition(privileged)\n",
    "\n",
    "        return utils.compute_num_pos_neg(\n",
    "            self.classified_dataset.protected_attributes,\n",
    "            self.classified_dataset.labels,\n",
    "            self.classified_dataset.instance_weights,\n",
    "            self.classified_dataset.protected_attribute_names,\n",
    "            self.classified_dataset.favorable_label,\n",
    "            condition=condition)\n",
    "\n",
    "    def num_pred_negatives(self, privileged=None):\n",
    "        r\"\"\":math:`\\sum_{i=1}^n \\mathbb{1}[\\hat{y}_i = \\text{unfavorable}]`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        condition = self._to_condition(privileged)\n",
    "\n",
    "        return utils.compute_num_pos_neg(\n",
    "            self.classified_dataset.protected_attributes,\n",
    "            self.classified_dataset.labels,\n",
    "            self.classified_dataset.instance_weights,\n",
    "            self.classified_dataset.protected_attribute_names,\n",
    "            self.classified_dataset.unfavorable_label,\n",
    "            condition=condition)\n",
    "\n",
    "    def selection_rate(self, privileged=None):\n",
    "        r\"\"\":math:`Pr(\\hat{Y} = \\text{favorable})`\n",
    "        Args:\n",
    "            privileged (bool, optional): Boolean prescribing whether to\n",
    "                condition this metric on the `privileged_groups`, if `True`, or\n",
    "                the `unprivileged_groups`, if `False`. Defaults to `None`\n",
    "                meaning this metric is computed over the entire dataset.\n",
    "        Raises:\n",
    "            AttributeError: `privileged_groups` or `unprivileged_groups` \n",
    "                must be provided at initialization to condition on them.\n",
    "        \"\"\"\n",
    "        return (self.num_pred_positives(privileged=privileged)\n",
    "              / self.num_instances(privileged=privileged))\n",
    "\n",
    "    def disparate_impact(self):\n",
    "        r\"\"\"\n",
    "        .. math::\n",
    "           \\frac{Pr(\\hat{Y} = 1 | D = \\text{unprivileged})}\n",
    "           {Pr(\\hat{Y} = 1 | D = \\text{privileged})}\n",
    "        \"\"\"\n",
    "        return self.ratio(self.selection_rate)\n",
    "\n",
    "    def statistical_parity_difference(self):\n",
    "        r\"\"\"\n",
    "        .. math::\n",
    "           Pr(\\hat{Y} = 1 | D = \\text{unprivileged})\n",
    "           - Pr(\\hat{Y} = 1 | D = \\text{privileged})\n",
    "        \"\"\"\n",
    "        return self.difference(self.selection_rate)\n",
    "\n",
    "    def generalized_entropy_index(self, alpha=2):\n",
    "        r\"\"\"Generalized entropy index is proposed as a unified individual and\n",
    "        group fairness measure in [3]_.  With :math:`b_i = \\hat{y}_i - y_i + 1`:\n",
    "        .. math::\n",
    "           \\mathcal{E}(\\alpha) = \\begin{cases}\n",
    "               \\frac{1}{n \\alpha (\\alpha-1)}\\sum_{i=1}^n\\left[\\left(\\frac{b_i}{\\mu}\\right)^\\alpha - 1\\right],& \\alpha \\ne 0, 1,\\\\\n",
    "               \\frac{1}{n}\\sum_{i=1}^n\\frac{b_{i}}{\\mu}\\ln\\frac{b_{i}}{\\mu},& \\alpha=1,\\\\\n",
    "               -\\frac{1}{n}\\sum_{i=1}^n\\ln\\frac{b_{i}}{\\mu},& \\alpha=0.\n",
    "           \\end{cases}\n",
    "        Args:\n",
    "            alpha (int): Parameter that regulates the weight given to distances\n",
    "                between values at different parts of the distribution.\n",
    "        References:\n",
    "            .. [3] T. Speicher, H. Heidari, N. Grgic-Hlaca, K. P. Gummadi, A. Singla, A. Weller, and M. B. Zafar,\n",
    "               \"A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual and Group Unfairness via Inequality Indices,\"\n",
    "               ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2018.\n",
    "        \"\"\"\n",
    "        y_pred = self.classified_dataset.labels.ravel()\n",
    "        y_true = self.dataset.labels.ravel()\n",
    "        y_pred = (y_pred == self.classified_dataset.favorable_label).astype(\n",
    "            np.float64)\n",
    "        y_true = (y_true == self.dataset.favorable_label).astype(np.float64)\n",
    "        b = 1 + y_pred - y_true\n",
    "\n",
    "        if alpha == 1:\n",
    "            # moving the b inside the log allows for 0 values\n",
    "            return np.mean(np.log((b / np.mean(b))**b) / np.mean(b))\n",
    "        elif alpha == 0:\n",
    "            return -np.mean(np.log(b / np.mean(b)) / np.mean(b))\n",
    "        else:\n",
    "            return np.mean((b / np.mean(b))**alpha - 1) / (alpha * (alpha - 1))\n",
    "\n",
    "    def _between_group_generalized_entropy_index(self, groups, alpha=2):\n",
    "        r\"\"\"Between-group generalized entropy index is proposed as a group\n",
    "        fairness measure in [2]_ and is one of two terms that the generalized\n",
    "        entropy index decomposes to.\n",
    "        Args:\n",
    "            groups (list): A list of groups over which to calculate this metric.\n",
    "                Groups should be disjoint. By default, this will use the\n",
    "                `privileged_groups` and `unprivileged_groups` as the only two\n",
    "                groups.\n",
    "            alpha (int): See :meth:`generalized_entropy_index`.\n",
    "        References:\n",
    "            .. [2] T. Speicher, H. Heidari, N. Grgic-Hlaca, K. P. Gummadi, A. Singla, A. Weller, and M. B. Zafar,\n",
    "               \"A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual and Group Unfairness via Inequality Indices,\"\n",
    "               ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2018.\n",
    "        \"\"\"\n",
    "        b = np.zeros(self.dataset.labels.size, dtype=np.float64)\n",
    "\n",
    "        for group in groups:\n",
    "            classified_group = utils.compute_boolean_conditioning_vector(\n",
    "                self.classified_dataset.protected_attributes,\n",
    "                self.classified_dataset.protected_attribute_names,\n",
    "                condition=group)\n",
    "            true_group = utils.compute_boolean_conditioning_vector(\n",
    "                self.dataset.protected_attributes,\n",
    "                self.dataset.protected_attribute_names,\n",
    "                condition=group)\n",
    "            # ignore if there are no members of this group present\n",
    "            if not np.any(true_group):\n",
    "                continue\n",
    "            y_pred = self.classified_dataset.labels[classified_group].ravel()\n",
    "            y_true = self.dataset.labels[true_group].ravel()\n",
    "            y_pred = (y_pred == self.classified_dataset.favorable_label).astype(\n",
    "                np.float64)\n",
    "            y_true = (y_true == self.dataset.favorable_label).astype(np.float64)\n",
    "            b[true_group] = np.mean(1 + y_pred - y_true)\n",
    "\n",
    "        if alpha == 1:\n",
    "            return np.mean(np.log((b / np.mean(b))**b) / np.mean(b))\n",
    "        elif alpha == 0:\n",
    "            return -np.mean(np.log(b / np.mean(b)) / np.mean(b))\n",
    "        else:\n",
    "            return np.mean((b / np.mean(b))**alpha - 1) / (alpha * (alpha - 1))\n",
    "\n",
    "    def between_all_groups_generalized_entropy_index(self, alpha=2):\n",
    "        \"\"\"Between-group generalized entropy index that uses all combinations of\n",
    "        groups based on `self.dataset.protected_attributes`. See\n",
    "        :meth:`_between_group_generalized_entropy_index`.\n",
    "        Args:\n",
    "            alpha (int): See :meth:`generalized_entropy_index`.\n",
    "        \"\"\"\n",
    "        all_values = list(map(np.concatenate, zip(\n",
    "            self.dataset.privileged_protected_attributes,\n",
    "            self.dataset.unprivileged_protected_attributes)))\n",
    "        groups = [[dict(zip(self.dataset.protected_attribute_names, vals))]\n",
    "                  for vals in product(*all_values)]\n",
    "        return self._between_group_generalized_entropy_index(groups=groups,\n",
    "            alpha=alpha)\n",
    "\n",
    "    def between_group_generalized_entropy_index(self, alpha=2):\n",
    "        \"\"\"Between-group generalized entropy index that uses\n",
    "        `self.privileged_groups` and `self.unprivileged_groups` as the only two\n",
    "        groups. See :meth:`_between_group_generalized_entropy_index`.\n",
    "        Args:\n",
    "            alpha (int): See :meth:`generalized_entropy_index`.\n",
    "        \"\"\"\n",
    "        groups = [self._to_condition(False), self._to_condition(True)]\n",
    "        return self._between_group_generalized_entropy_index(groups=groups,\n",
    "            alpha=alpha)\n",
    "\n",
    "    def theil_index(self):\n",
    "        r\"\"\"The Theil index is the :meth:`generalized_entropy_index` with\n",
    "        :math:`\\alpha = 1`.\n",
    "        \"\"\"\n",
    "        return self.generalized_entropy_index(alpha=1)\n",
    "\n",
    "    def coefficient_of_variation(self):\n",
    "        r\"\"\"The coefficient of variation is the square root of two times the\n",
    "        :meth:`generalized_entropy_index` with :math:`\\alpha = 2`.\n",
    "        \"\"\"\n",
    "        return np.sqrt(2*self.generalized_entropy_index(alpha=2))\n",
    "\n",
    "    def between_group_theil_index(self):\n",
    "        r\"\"\"The between-group Theil index is the\n",
    "        :meth:`between_group_generalized_entropy_index` with :math:`\\alpha = 1`.\n",
    "        \"\"\"\n",
    "        return self.between_group_generalized_entropy_index(alpha=1)\n",
    "\n",
    "    def between_group_coefficient_of_variation(self):\n",
    "        r\"\"\"The between-group coefficient of variation is the square\n",
    "        root of two times the :meth:`between_group_generalized_entropy_index` with\n",
    "        :math:`\\alpha = 2`.\n",
    "        \"\"\"\n",
    "        return np.sqrt(2*self.between_group_generalized_entropy_index(alpha=2))\n",
    "\n",
    "    def between_all_groups_theil_index(self):\n",
    "        r\"\"\"The between-group Theil index is the\n",
    "        :meth:`between_all_groups_generalized_entropy_index` with\n",
    "        :math:`\\alpha = 1`.\n",
    "        \"\"\"\n",
    "        return self.between_all_groups_generalized_entropy_index(alpha=1)\n",
    "\n",
    "    def between_all_groups_coefficient_of_variation(self):\n",
    "        r\"\"\"The between-group coefficient of variation is the square\n",
    "        root of two times the :meth:`between_all_groups_generalized_entropy_index` with\n",
    "        :math:`\\alpha = 2`.\n",
    "        \"\"\"\n",
    "        return np.sqrt(2*self.between_all_groups_generalized_entropy_index(\n",
    "            alpha=2))\n",
    "\n",
    "    def differential_fairness_bias_amplification(self, concentration=1.0):\n",
    "        \"\"\"Bias amplification is the difference in smoothed EDF between the\n",
    "        classifier and the original dataset. Positive values mean the bias\n",
    "        increased due to the classifier.\n",
    "        Args:\n",
    "            concentration (float, optional): Concentration parameter for\n",
    "                Dirichlet smoothing. Must be non-negative.\n",
    "        \"\"\"\n",
    "        ssr = self._smoothed_base_rates(self.classified_dataset.labels,\n",
    "                                        concentration)\n",
    "\n",
    "        def pos_ratio(i, j):\n",
    "            return abs(np.log(ssr[i]) - np.log(ssr[j]))\n",
    "\n",
    "        def neg_ratio(i, j):\n",
    "            return abs(np.log(1 - ssr[i]) - np.log(1 - ssr[j]))\n",
    "\n",
    "        edf_clf = max(max(pos_ratio(i, j), neg_ratio(i, j))\n",
    "                for i in range(len(ssr)) for j in range(len(ssr)) if i != j)\n",
    "        edf_data = self.smoothed_empirical_differential_fairness(concentration)\n",
    "\n",
    "        return edf_clf - edf_data\n",
    "\n",
    "    # ============================== ALIASES ===================================\n",
    "    def equal_opportunity_difference(self):\n",
    "        \"\"\"Alias of :meth:`true_positive_rate_difference`.\"\"\"\n",
    "        return self.true_positive_rate_difference()\n",
    "\n",
    "    def power(self, privileged=None):\n",
    "        \"\"\"Alias of :meth:`num_true_positives`.\"\"\"\n",
    "        return self.num_true_positives(privileged=privileged)\n",
    "\n",
    "    def precision(self, privileged=None):\n",
    "        \"\"\"Alias of :meth:`positive_predictive_value`.\"\"\"\n",
    "        return self.positive_predictive_value(privileged=privileged)\n",
    "\n",
    "    def recall(self, privileged=None):\n",
    "        \"\"\"Alias of :meth:`true_positive_rate`.\"\"\"\n",
    "        return self.true_positive_rate(privileged=privileged)\n",
    "\n",
    "    def sensitivity(self, privileged=None):\n",
    "        \"\"\"Alias of :meth:`true_positive_rate`.\"\"\"\n",
    "        return self.true_positive_rate(privileged=privileged)\n",
    "\n",
    "    def specificity(self, privileged=None):\n",
    "        \"\"\"Alias of :meth:`true_negative_rate`.\"\"\"\n",
    "        return self.true_negative_rate(privileged=privileged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqOddsPostprocessing(Transformer):\n",
    "    \"\"\"Equalized odds postprocessing is a post-processing technique that solves\n",
    "    a linear program to find probabilities with which to change output labels to\n",
    "    optimize equalized odds [8]_ [9]_.\n",
    "    References:\n",
    "        .. [8] M. Hardt, E. Price, and N. Srebro, \"Equality of Opportunity in\n",
    "           Supervised Learning,\" Conference on Neural Information Processing\n",
    "           Systems, 2016.\n",
    "        .. [9] G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and\n",
    "           K. Q. Weinberger, \"On Fairness and Calibration,\" Conference on Neural\n",
    "           Information Processing Systems, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unprivileged_groups, privileged_groups, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (list(dict)): Representation for unprivileged\n",
    "                group.\n",
    "            privileged_groups (list(dict)): Representation for privileged\n",
    "                group.\n",
    "            seed (int, optional): Seed to make `predict` repeatable.\n",
    "        \"\"\"\n",
    "        super(EqOddsPostprocessing, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups,\n",
    "            seed=seed)\n",
    "\n",
    "        self.seed = seed\n",
    "        self.model_params = None\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "\n",
    "    def fit(self, dataset_true, dataset_pred):\n",
    "        \"\"\"Compute parameters for equalizing odds using true and predicted\n",
    "        labels.\n",
    "        Args:\n",
    "            true_dataset (BinaryLabelDataset): Dataset containing true labels.\n",
    "            pred_dataset (BinaryLabelDataset): Dataset containing predicted\n",
    "                labels.\n",
    "        Returns:\n",
    "            EqOddsPostprocessing: Returns self.\n",
    "        \"\"\"\n",
    "        metric = ClassificationMetric(dataset_true, dataset_pred,\n",
    "            unprivileged_groups=self.unprivileged_groups,\n",
    "            privileged_groups=self.privileged_groups)\n",
    "\n",
    "        # compute basic statistics\n",
    "        sbr = metric.base_rate(privileged=True)\n",
    "        obr = metric.base_rate(privileged=False)\n",
    "\n",
    "        fpr0 = metric.false_positive_rate(privileged=True)\n",
    "        fpr1 = metric.false_positive_rate(privileged=False)\n",
    "        fnr0 = metric.false_negative_rate(privileged=True)\n",
    "        fnr1 = metric.false_negative_rate(privileged=False)\n",
    "        tpr0 = metric.true_positive_rate(privileged=True)\n",
    "        tpr1 = metric.true_positive_rate(privileged=False)\n",
    "        tnr0 = metric.true_negative_rate(privileged=True)\n",
    "        tnr1 = metric.true_negative_rate(privileged=False)\n",
    "\n",
    "        # linear program has 4 decision variables:\n",
    "        # [Pr[label_tilde = 1 | label_hat = 1, protected_attributes = 0];\n",
    "        #  Pr[label_tilde = 1 | label_hat = 0, protected_attributes = 0];\n",
    "        #  Pr[label_tilde = 1 | label_hat = 1, protected_attributes = 1];\n",
    "        #  Pr[label_tilde = 1 | label_hat = 0, protected_attributes = 1]]\n",
    "        # Coefficients of the linear objective function to be minimized.\n",
    "        c = np.array([fpr0 - tpr0, tnr0 - fnr0, fpr1 - tpr1, tnr1 - fnr1])\n",
    "\n",
    "        # A_ub - 2-D array which, when matrix-multiplied by x, gives the values\n",
    "        # of the upper-bound inequality constraints at x\n",
    "        # b_ub - 1-D array of values representing the upper-bound of each\n",
    "        # inequality constraint (row) in A_ub.\n",
    "        # Just to keep these between zero and one\n",
    "        A_ub = np.array([[ 1,  0,  0,  0],\n",
    "                         [-1,  0,  0,  0],\n",
    "                         [ 0,  1,  0,  0],\n",
    "                         [ 0, -1,  0,  0],\n",
    "                         [ 0,  0,  1,  0],\n",
    "                         [ 0,  0, -1,  0],\n",
    "                         [ 0,  0,  0,  1],\n",
    "                         [ 0,  0,  0, -1]], dtype=np.float64)\n",
    "        b_ub = np.array([1, 0, 1, 0, 1, 0, 1, 0], dtype=np.float64)\n",
    "\n",
    "        # Create boolean conditioning vectors for protected groups\n",
    "        cond_vec_priv = utils.compute_boolean_conditioning_vector(\n",
    "            dataset_pred.protected_attributes,\n",
    "            dataset_pred.protected_attribute_names,\n",
    "            self.privileged_groups)\n",
    "        cond_vec_unpriv = utils.compute_boolean_conditioning_vector(\n",
    "            dataset_pred.protected_attributes,\n",
    "            dataset_pred.protected_attribute_names,\n",
    "            self.unprivileged_groups)\n",
    "\n",
    "        sconst = np.ravel(\n",
    "            dataset_pred.labels[cond_vec_priv] == dataset_pred.favorable_label)\n",
    "        sflip = np.ravel(\n",
    "            dataset_pred.labels[cond_vec_priv] == dataset_pred.unfavorable_label)\n",
    "        oconst = np.ravel(\n",
    "            dataset_pred.labels[cond_vec_unpriv] == dataset_pred.favorable_label)\n",
    "        oflip = np.ravel(\n",
    "            dataset_pred.labels[cond_vec_unpriv] == dataset_pred.unfavorable_label)\n",
    "\n",
    "        y_true = dataset_true.labels.ravel()\n",
    "\n",
    "        sm_tn = np.logical_and(sflip,\n",
    "            y_true[cond_vec_priv] == dataset_true.unfavorable_label,\n",
    "            dtype=np.float64)\n",
    "        sm_fn = np.logical_and(sflip,\n",
    "            y_true[cond_vec_priv] == dataset_true.favorable_label,\n",
    "            dtype=np.float64)\n",
    "        sm_fp = np.logical_and(sconst,\n",
    "            y_true[cond_vec_priv] == dataset_true.unfavorable_label,\n",
    "            dtype=np.float64)\n",
    "        sm_tp = np.logical_and(sconst,\n",
    "            y_true[cond_vec_priv] == dataset_true.favorable_label,\n",
    "            dtype=np.float64)\n",
    "\n",
    "        om_tn = np.logical_and(oflip,\n",
    "            y_true[cond_vec_unpriv] == dataset_true.unfavorable_label,\n",
    "            dtype=np.float64)\n",
    "        om_fn = np.logical_and(oflip,\n",
    "            y_true[cond_vec_unpriv] == dataset_true.favorable_label,\n",
    "            dtype=np.float64)\n",
    "        om_fp = np.logical_and(oconst,\n",
    "            y_true[cond_vec_unpriv] == dataset_true.unfavorable_label,\n",
    "            dtype=np.float64)\n",
    "        om_tp = np.logical_and(oconst,\n",
    "            y_true[cond_vec_unpriv] == dataset_true.favorable_label,\n",
    "            dtype=np.float64)\n",
    "\n",
    "        # A_eq - 2-D array which, when matrix-multiplied by x,\n",
    "        # gives the values of the equality constraints at x\n",
    "        # b_eq - 1-D array of values representing the RHS of each equality\n",
    "        # constraint (row) in A_eq.\n",
    "        # Used to impose equality of odds constraint\n",
    "        A_eq = [[(np.mean(sconst*sm_tp) - np.mean(sflip*sm_tp)) / sbr,\n",
    "                 (np.mean(sflip*sm_fn) - np.mean(sconst*sm_fn)) / sbr,\n",
    "                 (np.mean(oflip*om_tp) - np.mean(oconst*om_tp)) / obr,\n",
    "                 (np.mean(oconst*om_fn) - np.mean(oflip*om_fn)) / obr],\n",
    "                [(np.mean(sconst*sm_fp) - np.mean(sflip*sm_fp)) / (1-sbr),\n",
    "                 (np.mean(sflip*sm_tn) - np.mean(sconst*sm_tn)) / (1-sbr),\n",
    "                 (np.mean(oflip*om_fp) - np.mean(oconst*om_fp)) / (1-obr),\n",
    "                 (np.mean(oconst*om_tn) - np.mean(oflip*om_tn)) / (1-obr)]]\n",
    "\n",
    "        b_eq = [(np.mean(oflip*om_tp) + np.mean(oconst*om_fn)) / obr\n",
    "              - (np.mean(sflip*sm_tp) + np.mean(sconst*sm_fn)) / sbr,\n",
    "                (np.mean(oflip*om_fp) + np.mean(oconst*om_tn)) / (1-obr)\n",
    "              - (np.mean(sflip*sm_fp) + np.mean(sconst*sm_tn)) / (1-sbr)]\n",
    "\n",
    "        # Linear program\n",
    "        self.model_params = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Perturb the predicted labels to obtain new labels that satisfy\n",
    "        equalized odds constraints.\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing labels that needs\n",
    "                to be transformed.\n",
    "            dataset (BinaryLabelDataset): Transformed dataset.\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # Get the model parameters output from fit\n",
    "        sp2p, sn2p, op2p, on2p = self.model_params.x\n",
    "\n",
    "        # Create boolean conditioning vectors for protected groups\n",
    "        cond_vec_priv = utils.compute_boolean_conditioning_vector(\n",
    "            dataset.protected_attributes, dataset.protected_attribute_names,\n",
    "            self.privileged_groups)\n",
    "        cond_vec_unpriv = utils.compute_boolean_conditioning_vector(\n",
    "            dataset.protected_attributes, dataset.protected_attribute_names,\n",
    "            self.unprivileged_groups)\n",
    "\n",
    "        # Randomly flip labels according to the probabilities in model_params\n",
    "        self_fair_pred = dataset.labels[cond_vec_priv].copy()\n",
    "        self_pp_indices, _ = np.nonzero(\n",
    "            dataset.labels[cond_vec_priv] == dataset.favorable_label)\n",
    "        self_pn_indices, _ = np.nonzero(\n",
    "            dataset.labels[cond_vec_priv] == dataset.unfavorable_label)\n",
    "        np.random.shuffle(self_pp_indices)\n",
    "        np.random.shuffle(self_pn_indices)\n",
    "\n",
    "        n2p_indices = self_pn_indices[:int(len(self_pn_indices) * sn2p)]\n",
    "        self_fair_pred[n2p_indices] = dataset.favorable_label\n",
    "        p2n_indices = self_pp_indices[:int(len(self_pp_indices) * (1 - sp2p))]\n",
    "        self_fair_pred[p2n_indices] = dataset.unfavorable_label\n",
    "\n",
    "        othr_fair_pred = dataset.labels[cond_vec_unpriv].copy()\n",
    "        othr_pp_indices, _ = np.nonzero(\n",
    "            dataset.labels[cond_vec_unpriv] == dataset.favorable_label)\n",
    "        othr_pn_indices, _ = np.nonzero(\n",
    "            dataset.labels[cond_vec_unpriv] == dataset.unfavorable_label)\n",
    "        np.random.shuffle(othr_pp_indices)\n",
    "        np.random.shuffle(othr_pn_indices)\n",
    "\n",
    "        n2p_indices = othr_pn_indices[:int(len(othr_pn_indices) * on2p)]\n",
    "        othr_fair_pred[n2p_indices] = dataset.favorable_label\n",
    "        p2n_indices = othr_pp_indices[:int(len(othr_pp_indices) * (1 - op2p))]\n",
    "        othr_fair_pred[p2n_indices] = dataset.unfavorable_label\n",
    "\n",
    "        # Mutated, fairer dataset with new labels\n",
    "        dataset_new = dataset.copy()\n",
    "\n",
    "        new_labels = np.zeros_like(dataset.labels, dtype=np.float64)\n",
    "        new_labels[cond_vec_priv] = self_fair_pred\n",
    "        new_labels[cond_vec_unpriv] = othr_fair_pred\n",
    "\n",
    "        dataset_new.labels = new_labels\n",
    "\n",
    "        return dataset_new\n",
    "\n",
    "    def fit_predict(self, dataset_true, dataset_pred):\n",
    "        \"\"\"fit and predict methods sequentially.\"\"\"\n",
    "        return self.fit(dataset_true, dataset_pred).predict(dataset_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
